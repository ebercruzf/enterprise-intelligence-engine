# üìã Enterprise-intelligence-engine
Public - Requirements Configuration Guide

## üéØ Estrategia de Configuraci√≥n Flexible

Este `requirements.txt` est√° dise√±ado para soportar m√∫ltiples configuraciones:
- **üè† Stack 100% Local:** Sin dependencias cloud
- **‚òÅÔ∏è Stack H√≠brido:** Local + algunos servicios cloud  
- **üåê Stack Full Cloud:** APIs cloud para LLM y embeddings

## üîß Configuraciones por Casos de Uso

### üì¶ **Configuraci√≥n 1: Stack Completamente Local (Tu caso actual)**

```bash
# Instalar solo dependencias locales
pip install langchain==0.1.0 langchain-community==0.0.13
pip install weaviate-client==3.25.3
pip install sentence-transformers==2.2.2 torch==2.1.1 transformers==4.36.0
pip install requests==2.31.0 httpx==0.25.2
pip install streamlit==1.29.0
pip install pypdf2==3.0.1 python-docx==0.8.11 openpyxl==3.1.2
pip install pandas==2.1.4 numpy==1.24.3 matplotlib==3.7.1 plotly==5.17.0
pip install python-dotenv==1.0.0 pydantic==2.5.0 pyyaml==6.0.1
```

### üì¶ **Configuraci√≥n 2: Local + OpenAI para Emergencias**

```bash
# Agregar a la configuraci√≥n local:
pip install openai==1.6.1
```

Entonces en tu c√≥digo:
```python
class LLMProvider(Enum):
    DEEPSEEK_API = "deepseek_api"
    DEEPSEEK_DIRECT = "deepseek_direct"  
    OPENAI = "openai"                    # ‚úÖ Ahora disponible

class EmbeddingProvider(Enum):
    SENTENCE_BERT = "sentence_bert"
    OPENAI = "openai"                    # ‚úÖ Ahora disponible
```

### üì¶ **Configuraci√≥n 3: H√≠brido con Vector DB Cloud**

```bash
# Todo lo local + Pinecone cloud para escalabilidad
pip install pinecone-client==2.2.4
```

### üì¶ **Configuraci√≥n 4: Full Cloud para Producci√≥n**

```bash
# Instalar todas las opciones cloud
pip install openai==1.6.1 anthropic==0.8.1 cohere==4.37
pip install pinecone-client==2.2.4 qdrant-client==1.6.9
```

## üöÄ Archivos de Requirements Modulares

### `requirements-base.txt` (Siempre necesario)
```txt
langchain==0.1.0
langchain-community==0.0.13
streamlit==1.29.0
requests==2.31.0
pandas==2.1.4
python-dotenv==1.0.0
```

### `requirements-local.txt` (Para stack local)
```txt
-r requirements-base.txt
weaviate-client==3.25.3
sentence-transformers==2.2.2
torch==2.1.1
```

### `requirements-cloud.txt` (Para servicios cloud)
```txt
-r requirements-base.txt
openai==1.6.1
pinecone-client==2.2.4
anthropic==0.8.1
```

### `requirements-dev.txt` (Para desarrollo)
```txt
-r requirements-local.txt
pytest==7.4.3
black==23.11.0
flake8==6.1.0
```

## üìù Instalaci√≥n Seg√∫n tu Necesidad

### **M√©todo 1: Todo en uno (recomendado para desarrollo)**
```bash
pip install -r requirements.txt
```

### **M√©todo 2: Modular**
```bash
# Solo lo b√°sico + local
pip install -r requirements-local.txt

# Agregar cloud cuando necesites
pip install -r requirements-cloud.txt
```

### **M√©todo 3: Selective Install**
```bash
# Crear tu propio requirements
cat > my-requirements.txt << EOF
-r requirements-base.txt
weaviate-client==3.25.3    # Tu vector DB
sentence-transformers==2.2.2  # Tus embeddings
openai==1.6.1             # Backup LLM
EOF

pip install -r my-requirements.txt
```

## üîÑ Actualizaci√≥n del C√≥digo para Soportar Cloud

### **Configuraci√≥n Actualizada (`config.py`)**
```python
class LLMProvider(Enum):
    # Local
    DEEPSEEK_API = "deepseek_api"
    DEEPSEEK_DIRECT = "deepseek_direct"
    
    # Cloud (descomenta cuando necesites)
    # OPENAI = "openai"
    # ANTHROPIC = "anthropic"
    # COHERE = "cohere"

class VectorDBProvider(Enum):
    # Local  
    WEAVIATE = "weaviate"
    CHROMA = "chroma"
    
    # Cloud (descomenta cuando necesites)
    # PINECONE = "pinecone"
    # QDRANT = "qdrant"

@dataclass
class RAGConfig:
    # Providers
    llm_provider: LLMProvider = LLMProvider.DEEPSEEK_API
    vector_db_provider: VectorDBProvider = VectorDBProvider.WEAVIATE
    embedding_provider: EmbeddingProvider = EmbeddingProvider.SENTENCE_BERT
    
    # URLs locales
    weaviate_url: str = "http://localhost:8080"
    deepseek_api_url: str = "http://localhost:11004/api/llm/rag-query"
    
    # API Keys cloud (opcional)
    openai_api_key: Optional[str] = None
    pinecone_api_key: Optional[str] = None
    anthropic_api_key: Optional[str] = None
```

## üéõÔ∏è Variables de Entorno Flexibles

### `.env` actualizado
```bash
# =============================================================================
# CONFIGURACI√ìN FLEXIBLE - Banking RAG System
# =============================================================================

# Providers activos
LLM_PROVIDER=deepseek_api              # deepseek_api, deepseek_direct, openai
VECTOR_DB_PROVIDER=weaviate            # weaviate, chroma, pinecone
EMBEDDING_PROVIDER=sentence_bert       # sentence_bert, openai

# Local URLs
WEAVIATE_URL=http://localhost:8080
DEEPSEEK_API_URL=http://localhost:11004/api/llm/rag-query
DEEPSEEK_DIRECT_URL=http://localhost:11434

# Cloud API Keys (opcional - solo llenar si usas)
OPENAI_API_KEY=
PINECONE_API_KEY=
ANTHROPIC_API_KEY=
COHERE_API_KEY=

# Configuraci√≥n general
RAG_TEMPERATURE=0.2
RAG_CHUNK_SIZE=1000
RAG_MAX_TOKENS=2000
```

## üìä Ventajas de esta Estructura

### ‚úÖ **Flexibilidad Total**
- Puedes empezar 100% local
- Agregar cloud incrementalmente
- Cambiar providers sin reescribir c√≥digo

### ‚úÖ **Gesti√≥n de Costos**
- Local = $0 despu√©s de setup inicial
- Cloud = Solo pagas lo que usas

### ‚úÖ **Escalabilidad**
- Local para desarrollo/testing
- Cloud para producci√≥n/volumen alto

### ‚úÖ **Redundancia**
- Si tu API local falla ‚Üí autom√°ticamente usa cloud
- M√∫ltiples providers como backup

## üö® Recomendaciones de Seguridad

### **Para Producci√≥n**
```bash
# Nunca commites API keys
echo "*.env" >> .gitignore
echo ".env.local" >> .gitignore
echo ".env.production" >> .gitignore

# Usa archivos separados por ambiente
.env.development     # Local only
.env.staging        # Local + some cloud
.env.production     # Full cloud
```

### **Verificaci√≥n de Configuraci√≥n**
```python
# Agregar al inicio de tu app
def verify_config(config: RAGConfig):
    if config.llm_provider == LLMProvider.OPENAI and not config.openai_api_key:
        raise ValueError("OpenAI API key requerida para provider OpenAI")
    
    if config.vector_db_provider == VectorDBProvider.PINECONE and not config.pinecone_api_key:
        raise ValueError("Pinecone API key requerida para provider Pinecone")
```

## üèÅ Pr√≥ximos Pasos

1. **Ahora:** Usa la configuraci√≥n local completa
2. **M√°s tarde:** Descomenta solo lo que necesites
3. **Producci√≥n:** Eval√∫a qu√© partes migrar a cloud seg√∫n volumen/costo

¬øTe parece bien esta estructura flexible?